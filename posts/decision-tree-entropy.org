#+BEGIN_COMMENT
.. title: Decision Tree Entropy
.. slug: decision-tree-entropy
.. date: 2017-03-06 14:14:02 UTC-08:00
.. tags: machineLearning entropy decisionTrees
.. category: learning machinelearning
.. link: 
.. description: How entropy is calculated.
.. type: text
#+END_COMMENT

These are some basic notes on how entropy is used when making decision trees. The examples are taken from /Process Mining: Data Science In Action/.

* Entropy

The equation for entropy is $E = - \sum\limits_{i=1}^n p_i \log_2 p_i$. $p_i$ is the probability of variable $i$. In other words, for variable $i$, $p_i$ is the count of instances for that variable divided by the total number of instances for all variables. Well, I'm probably not saying this clearly enough. A concrete example might be better.


