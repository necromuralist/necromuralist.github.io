<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Cloistered Monkey (Posts about networking)</title><link>https://necromuralist.github.io/</link><description></description><atom:link href="https://necromuralist.github.io/categories/networking.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Mon, 07 Jan 2019 20:25:31 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Tuning a TCP Connection</title><link>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgc930d9f"&gt;TCP Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgbf8b50d"&gt;The TCP Congestion Window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org5c4d045"&gt;Optimal Buffer Size&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org4615bf6"&gt;Changing the Buffer Size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgfc89cb5"&gt;Picking a Buffer Size&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgebf31f1"&gt;The Maximum Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgfee4da5"&gt;Bandwidth Delay Product&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org622806f"&gt;Iperf Tuning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgb6daa75"&gt;Iperf Warnings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org1dc6cba"&gt;Parallel Streams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org5501262"&gt;Maximum Transmission Unit (MTU)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
I wrote this a long time ago so I don't remember the source or really the intent. I think it was about trying to maximize the throughput when running iperf over TCP.
&lt;/p&gt;

&lt;div id="outline-container-orgc930d9f" class="outline-2"&gt;
&lt;h2 id="orgc930d9f"&gt;TCP Background&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc930d9f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbf8b50d" class="outline-3"&gt;
&lt;h3 id="orgbf8b50d"&gt;The TCP Congestion Window&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbf8b50d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Larger window implies higher throughput&lt;/li&gt;
&lt;li&gt;The max allowed is based on the buffer space the kernel allocates for
each socket&lt;/li&gt;
&lt;li&gt;Each socket has a default buffer size&lt;/li&gt;
&lt;li&gt;Sender and receiver can both adjust the size&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5c4d045" class="outline-3"&gt;
&lt;h3 id="org5c4d045"&gt;Optimal Buffer Size&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5c4d045"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Too small - Sender will be throttled
&lt;ul class="org-ul"&gt;
&lt;li&gt;More likely if the Sender is faster than the receiver&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Too big - Receiver might be overloaded and packets will be dropped
&lt;ul class="org-ul"&gt;
&lt;li&gt;If the receiver has lots of memory, less likely to happen&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4615bf6" class="outline-2"&gt;
&lt;h2 id="org4615bf6"&gt;Changing the Buffer Size&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4615bf6"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;TCP chooses the smaller of both sides of the transaction.&lt;/li&gt;
&lt;li&gt;Set the receiver high and let the sender negotiate&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfc89cb5" class="outline-2"&gt;
&lt;h2 id="orgfc89cb5"&gt;Picking a Buffer Size&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfc89cb5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgebf31f1" class="outline-3"&gt;
&lt;h3 id="orgebf31f1"&gt;The Maximum Throughput&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgebf31f1"&gt;
&lt;p&gt;
If the network isn't congested the throughput is characterized by TCP buffer size and network latency.
&lt;/p&gt;

\begin{equation*} max \; throughput \gets \frac{buffer \; size}{latency}
\end{equation*}
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfee4da5" class="outline-3"&gt;
&lt;h3 id="orgfee4da5"&gt;Bandwidth Delay Product&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfee4da5"&gt;
&lt;p&gt;
This is a rule of thumb to calculate the optimal buffer size.
&lt;/p&gt;

\begin{equation*} bdp \gets bottleneck \; bandwidth \times round \; trip
\; time \end{equation*}

&lt;ul class="org-ul"&gt;
&lt;li&gt;Get the &lt;b&gt;rtt&lt;/b&gt; from &lt;b&gt;ping&lt;/b&gt;.&lt;/li&gt;
&lt;li&gt;Get &lt;b&gt;bottleneck bandwidth&lt;/b&gt; from the theoretical maximum rate of the slowest link.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org622806f" class="outline-2"&gt;
&lt;h2 id="org622806f"&gt;Iperf Tuning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org622806f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb6daa75" class="outline-3"&gt;
&lt;h3 id="orgb6daa75"&gt;Iperf Warnings&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb6daa75"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Iperf can set the buffer size up to a point – but the OS sets the
upper limit on window sizes.&lt;/li&gt;
&lt;li&gt;If the request is too high, iperf will use the maximum allowed and gives a warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1dc6cba" class="outline-3"&gt;
&lt;h3 id="org1dc6cba"&gt;Parallel Streams&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1dc6cba"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Iperf lets you run multiple parallel sessions using the -P flag.&lt;/li&gt;

&lt;li&gt;If the aggregate (SUM) is greater than a single stream, this is an indication that something is wrong – most likely one of:
&lt;ul class="org-ul"&gt;
&lt;li&gt;The TCP window is too small&lt;/li&gt;
&lt;li&gt;The OS implementation has bugs&lt;/li&gt;
&lt;li&gt;The network has problems&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5501262" class="outline-3"&gt;
&lt;h3 id="org5501262"&gt;Maximum Transmission Unit (MTU)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5501262"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;the most effective way to set it is if both hosts support Path MTU Discovery and set it themselves.&lt;/li&gt;
&lt;li&gt;iperf's -m flag displays what mss is being used&lt;/li&gt;
&lt;li&gt;&lt;b&gt;mss&lt;/b&gt;: &lt;i&gt;Maximum Segment Size&lt;/i&gt; – the maximum TCP segment size&lt;/li&gt;
&lt;/ul&gt;

\begin{equation*} mss \gets MTU - protocol \; headers \end{equation*}

&lt;ul class="org-ul"&gt;
&lt;li&gt;Using -m is mainly to watch for the warning that the node lacks path mtu discovery&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>networking</category><category>optimization</category><guid>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</guid><pubDate>Mon, 10 Sep 2012 06:51:00 GMT</pubDate></item></channel></rss>