<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Cloistered Monkey (Posts about Networking)</title><link>https://necromuralist.github.io/</link><description></description><atom:link href="https://necromuralist.github.io/categories/networking.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 14 Jan 2018 22:28:53 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Tuning a TCP Connection</title><link>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div class="document" id="tuning-a-tcp-connection"&gt;&lt;div class="section" id="tcp-background"&gt;&lt;h4&gt;TCP Background&lt;/h4&gt;&lt;div class="section" id="the-tcp-congestion-window"&gt;&lt;h5&gt;The TCP Congestion Window&lt;/h5&gt;&lt;ul class="simple"&gt;&lt;li&gt;Larger window implies higher throughput&lt;/li&gt;&lt;li&gt;The max allowed is based on the buffer space the kernel allocates for each socket&lt;/li&gt;&lt;li&gt;Each socket has a default buffer size&lt;/li&gt;&lt;li&gt;Sender and receiver can both adjust the size&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class="section" id="optimal-buffer-size"&gt;&lt;h5&gt;Optimal Buffer Size&lt;/h5&gt;&lt;ul&gt;&lt;li&gt;&lt;div class="first"&gt;Too small -- Sender will be throttled&lt;/div&gt;&lt;/li&gt;&lt;li&gt;&lt;div class="first"&gt;Too big -- receiver might be overloaded and packets will be dropped&lt;/div&gt;&lt;blockquote&gt;&lt;ul class="simple"&gt;&lt;li&gt;More likely if the Sender is faster than the receiver&lt;/li&gt;&lt;li&gt;If the receiver has lots of memory, less likely to happen&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="section" id="changing-the-buffer-size"&gt;&lt;h4&gt;Changing the Buffer Size&lt;/h4&gt;&lt;ul class="simple"&gt;&lt;li&gt;TCP chooses the smaller of both sides of the transaction.&lt;/li&gt;&lt;li&gt;Set the receiver high and let the sender negotiate&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class="section" id="picking-a-buffer-size"&gt;&lt;h4&gt;Picking a Buffer Size&lt;/h4&gt;&lt;div class="section" id="the-maximum-throughput"&gt;&lt;h5&gt;The Maximum Throughput&lt;/h5&gt;If the network isn't congested the throughput is characterized by TCP buffer size and network latency.&lt;br&gt;&lt;div class="math"&gt;\begin{equation*} max \; throughput \gets \frac{buffer \; size}{latency} \end{equation*} &lt;/div&gt;&lt;/div&gt;&lt;div class="section" id="bandwidth-delay-product"&gt;&lt;h5&gt;Bandwidth Delay Product&lt;/h5&gt;This is a rule of thumb to calculate the optimal buffer size.&lt;br&gt;&lt;div class="math"&gt;\begin{equation*} bdp \gets bottleneck \; bandwidth \times round \; trip \; time \end{equation*} &lt;/div&gt;&lt;div class="math"&gt;\begin{equation*} * Get the *rtt* from *ping* \end{equation*} &lt;/div&gt;&lt;div class="math"&gt;\begin{equation*} * Get *bottleneck bandwidth* from the theoretical  maximum rate of the slowest link. \end{equation*} &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="section" id="iperf-tuning"&gt;&lt;h4&gt;Iperf Tuning&lt;/h4&gt;&lt;div class="section" id="iperf-warnings"&gt;&lt;h5&gt;Iperf Warnings&lt;/h5&gt;&lt;ul class="simple"&gt;&lt;li&gt;Iperf can set the buffer size up to a point -- but the OS sets the upper limit on window sizes.&lt;/li&gt;&lt;li&gt;If the request is too high, iperf will use the maximum allowed and gives a warning.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class="section" id="parallel-streams"&gt;&lt;h5&gt;Parallel Streams&lt;/h5&gt;&lt;ul&gt;&lt;li&gt;&lt;div class="first"&gt;Iperf lets you run multiple parallel sessions using the &lt;cite&gt;-P&lt;/cite&gt; flag.&lt;/div&gt;&lt;/li&gt;&lt;li&gt;&lt;div class="first"&gt;If the aggregate (SUM) is greater than a single stream, this is an indication that something is wrong -- most likely one of:&lt;/div&gt;&lt;blockquote&gt;&lt;ul class="simple"&gt;&lt;li&gt;The TCP window is too small&lt;/li&gt;&lt;li&gt;The OS implementation has bugs&lt;/li&gt;&lt;li&gt;The network has problems&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class="section" id="maximum-transmission-unit-mtu"&gt;&lt;h5&gt;Maximum Transmission Unit (MTU)&lt;/h5&gt;&lt;div class="system-message"&gt;&lt;div class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;&amp;lt;string&amp;gt;&lt;/tt&gt;, line 85)&lt;/div&gt;Title underline too short.&lt;br&gt;&lt;pre class="literal-block"&gt;Maximum Transmission Unit (MTU)&lt;br&gt;------------------------------&lt;br&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul class="simple"&gt;&lt;li&gt;the most effective way to set it is if both hosts support &lt;cite&gt;Path MTU Discovery&lt;/cite&gt; and set it themselves.&lt;/li&gt;&lt;li&gt;iperf's &lt;cite&gt;-m&lt;/cite&gt; flag displays what &lt;cite&gt;mss&lt;/cite&gt; is being used&lt;/li&gt;&lt;li&gt;&lt;strong&gt;mss&lt;/strong&gt;: &lt;em&gt;Maximum Segment Size&lt;/em&gt; -- the maximum TCP segment size&lt;/li&gt;&lt;/ul&gt;&lt;div class="math"&gt;\begin{equation*} mss \gets MTU - protocol \; headers \end{equation*} &lt;/div&gt;&lt;ul class="simple"&gt;&lt;li&gt;Using &lt;cite&gt;-m&lt;/cite&gt; is mainly to watch for the warning that the node lacks &lt;cite&gt;path mtu discovery&lt;/cite&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Networking</category><category>Optimization</category><guid>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</guid><pubDate>Mon, 10 Sep 2012 06:51:00 GMT</pubDate></item></channel></rss>