<html><body><div class="document" id="tuning-a-tcp-connection"><div class="section" id="tcp-background"><h4>TCP Background</h4><div class="section" id="the-tcp-congestion-window"><h5>The TCP Congestion Window</h5><ul class="simple"><li>Larger window implies higher throughput</li><li>The max allowed is based on the buffer space the kernel allocates for each socket</li><li>Each socket has a default buffer size</li><li>Sender and receiver can both adjust the size</li></ul></div><div class="section" id="optimal-buffer-size"><h5>Optimal Buffer Size</h5><ul><li><div class="first">Too small -- Sender will be throttled</div></li><li><div class="first">Too big -- receiver might be overloaded and packets will be dropped</div><blockquote><ul class="simple"><li>More likely if the Sender is faster than the receiver</li><li>If the receiver has lots of memory, less likely to happen</li></ul></blockquote></li></ul></div></div><div class="section" id="changing-the-buffer-size"><h4>Changing the Buffer Size</h4><ul class="simple"><li>TCP chooses the smaller of both sides of the transaction.</li><li>Set the receiver high and let the sender negotiate</li></ul></div><div class="section" id="picking-a-buffer-size"><h4>Picking a Buffer Size</h4><div class="section" id="the-maximum-throughput"><h5>The Maximum Throughput</h5>If the network isn't congested the throughput is characterized by TCP buffer size and network latency.<br><div class="math">\begin{equation*} max \; throughput \gets \frac{buffer \; size}{latency} \end{equation*} </div></div><div class="section" id="bandwidth-delay-product"><h5>Bandwidth Delay Product</h5>This is a rule of thumb to calculate the optimal buffer size.<br><div class="math">\begin{equation*} bdp \gets bottleneck \; bandwidth \times round \; trip \; time \end{equation*} </div><div class="math">\begin{equation*} * Get the *rtt* from *ping* \end{equation*} </div><div class="math">\begin{equation*} * Get *bottleneck bandwidth* from the theoretical  maximum rate of the slowest link. \end{equation*} </div></div></div><div class="section" id="iperf-tuning"><h4>Iperf Tuning</h4><div class="section" id="iperf-warnings"><h5>Iperf Warnings</h5><ul class="simple"><li>Iperf can set the buffer size up to a point -- but the OS sets the upper limit on window sizes.</li><li>If the request is too high, iperf will use the maximum allowed and gives a warning.</li></ul></div><div class="section" id="parallel-streams"><h5>Parallel Streams</h5><ul><li><div class="first">Iperf lets you run multiple parallel sessions using the <cite>-P</cite> flag.</div></li><li><div class="first">If the aggregate (SUM) is greater than a single stream, this is an indication that something is wrong -- most likely one of:</div><blockquote><ul class="simple"><li>The TCP window is too small</li><li>The OS implementation has bugs</li><li>The network has problems</li></ul></blockquote></li></ul></div><div class="section" id="maximum-transmission-unit-mtu"><h5>Maximum Transmission Unit (MTU)</h5><div class="system-message"><div class="system-message-title">System Message: WARNING/2 (<tt class="docutils">&lt;string&gt;</tt>, line 85)</div>Title underline too short.<br><pre class="literal-block">Maximum Transmission Unit (MTU)<br>------------------------------<br></pre></div><ul class="simple"><li>the most effective way to set it is if both hosts support <cite>Path MTU Discovery</cite> and set it themselves.</li><li>iperf's <cite>-m</cite> flag displays what <cite>mss</cite> is being used</li><li><strong>mss</strong>: <em>Maximum Segment Size</em> -- the maximum TCP segment size</li></ul><div class="math">\begin{equation*} mss \gets MTU - protocol \; headers \end{equation*} </div><ul class="simple"><li>Using <cite>-m</cite> is mainly to watch for the warning that the node lacks <cite>path mtu discovery</cite></li></ul></div></div></div></body></html>