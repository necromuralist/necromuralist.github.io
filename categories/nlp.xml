<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Cloistered Monkey (Posts about nlp)</title><link>https://necromuralist.github.io/</link><description></description><atom:link href="https://necromuralist.github.io/categories/nlp.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2023 &lt;a href="mailto:cloisteredmonkey.jmark@slmail.me"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Thu, 18 May 2023 07:55:24 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Precision, Recall, and the F-Measure</title><link>https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#org87625ea"&gt;Beginning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#org87e04dc"&gt;The Metrics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#org2f21d21"&gt;Positive and Negative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#orgfa8d509"&gt;Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#orgf675193"&gt;Precision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#org28e9cf4"&gt;Recall&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#org2877c20"&gt;F-Measure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#org777549e"&gt;F1 Measure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/#orgca6a179"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org87625ea" class="outline-2"&gt;
&lt;h2 id="org87625ea"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org87625ea"&gt;
&lt;p&gt;
When we are looking at how well a model (or a person) is doing it's often best to have a numeric value that we can calculate to make it easy to see how well it is doing. The first thing many people reach for is measuring &lt;i&gt;accuracy&lt;/i&gt; but this isn't always the best metric. Unbalanced data sets can distort this metric, for instance. If 90% of the data is spam then a model that always guessed that an email is spam will have decent accuracy, but really won't be all that useful (except for pointing out that you have too much spam). To remedy this and other problems I'll look at some alternative metrics (&lt;i&gt;precision&lt;/i&gt;, &lt;i&gt;recall&lt;/i&gt;, and the &lt;i&gt;f-measure&lt;/i&gt;) which are useful for deciding how well classification models are doing.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org87e04dc" class="outline-2"&gt;
&lt;h2 id="org87e04dc"&gt;The Metrics&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org87e04dc"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2f21d21" class="outline-3"&gt;
&lt;h3 id="org2f21d21"&gt;Positive and Negative&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2f21d21"&gt;
&lt;p&gt;
First some terminology. We're going to assume that we want to label data as either being something or not being that thing. e.g. guilty or not guilty, duck or not a duck, etc. The label for things that are the thing is called &lt;i&gt;Positive&lt;/i&gt; and the label for things that aren't the thing is &lt;i&gt;Negative&lt;/i&gt;.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Term&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Acronym&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;True Positive&lt;/td&gt;
&lt;td class="org-left"&gt;TP&lt;/td&gt;
&lt;td class="org-left"&gt;We labeled it positive and it was positive&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;False Positive&lt;/td&gt;
&lt;td class="org-left"&gt;FP&lt;/td&gt;
&lt;td class="org-left"&gt;We labeled it positive and it was negative&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;True Negative&lt;/td&gt;
&lt;td class="org-left"&gt;TN&lt;/td&gt;
&lt;td class="org-left"&gt;We labeled it negative and it was negative&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;False Negative&lt;/td&gt;
&lt;td class="org-left"&gt;FN&lt;/td&gt;
&lt;td class="org-left"&gt;We labeled it negative and it was positive&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
This is sometimes represented using a matrix.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt; &lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Actually Positive&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Actually Negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;b&gt;Predicted Positive&lt;/b&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;&lt;i&gt;True Positive&lt;/i&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;&lt;i&gt;False Positive&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;b&gt;Predicted Negative&lt;/b&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;&lt;i&gt;False Negative&lt;/i&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;&lt;i&gt;True Negative&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfa8d509" class="outline-3"&gt;
&lt;h3 id="orgfa8d509"&gt;Accuracy&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfa8d509"&gt;
&lt;p&gt;
Okay, I said we aren't going to use accuracy, but just to be complete… accuracy asks &lt;i&gt;what fraction of the anwsers did you get correct?&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;
\[
\textrm{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]
&lt;/p&gt;

&lt;p&gt;
This is probably what most of us are familiar with from being graded in school.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf675193" class="outline-3"&gt;
&lt;h3 id="orgf675193"&gt;Precision&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf675193"&gt;
&lt;p&gt;
&lt;i&gt;How much of what was predicted positive was really positive?&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;
\[
\textrm{Precision} = \frac{TP}{TP+FP}
\]
&lt;/p&gt;

&lt;p&gt;
Since we have the count of false-positives in the denominator, your score will go down the more negatives you label positive (e.g. the more innocents you convict, the lower the score).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org28e9cf4" class="outline-3"&gt;
&lt;h3 id="org28e9cf4"&gt;Recall&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org28e9cf4"&gt;
&lt;p&gt;
&lt;i&gt;How many of the positives did you catch?&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;
\[
\textrm{Recall} = \frac{TP}{TP + FN}
\]
&lt;/p&gt;

&lt;p&gt;
Here your score will go down the more positives you miss (the more guilty you find innocent).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2877c20" class="outline-3"&gt;
&lt;h3 id="org2877c20"&gt;F-Measure&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2877c20"&gt;
&lt;p&gt;
So, in some cases you might want to favor &lt;i&gt;Precision&lt;/i&gt; over &lt;i&gt;Recall&lt;/i&gt; and vice-versa, but what if you don't really want one over the other? The &lt;i&gt;F-Measure&lt;/i&gt; allows us to combine them into one metric.
&lt;/p&gt;

&lt;p&gt;
\[
F_{\beta} = \frac{(\beta^2 + 1) Precision \times Recall}{\beta^2 Precision + Recall}
\]
&lt;/p&gt;

&lt;p&gt;
To make it simpler I'll just use &lt;i&gt;P&lt;/i&gt; for precision and &lt;i&gt;R&lt;/i&gt; for recall from here on.
&lt;/p&gt;

&lt;p&gt;
\(\beta\) in the equation is a parameter that we can tune to favor precision or recall. If you'll notice, \(\beta\) in the numerator affects both precision and recall equally, while it only affects precision in the denominator, so the larger it is, the more precision diminishes the score.
&lt;/p&gt;

\begin{align}
\beta &amp;gt; 1 &amp;amp;: \textit{Favor Recall}\\
\beta &amp;lt; 1 &amp;amp;: \textit{Favor Precision}\\
\end{align}
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org777549e" class="outline-3"&gt;
&lt;h3 id="org777549e"&gt;F1 Measure&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org777549e"&gt;
&lt;p&gt;
If you look at the inequalities for the effects of \(\beta\) on the F-Measure you might notice that they don't include 1. That's because when \(\beta\) is 1 it doesn't favor either precision or recall, giving a case that combines both of them and treating them equally.
&lt;/p&gt;

&lt;p&gt;
\[
F_1 = \frac{2PR}{P + R}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgca6a179" class="outline-2"&gt;
&lt;h2 id="orgca6a179"&gt;References&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgca6a179"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/bib-speech-and-language-processing-jurafsky-martin/"&gt;Speech and Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/wikipedia-on-f1-score/"&gt;Wikipedia on the F1 Score&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>metrics</category><category>nlp</category><category>slipnote</category><category>statistics</category><guid>https://necromuralist.github.io/posts/precision-recall-and-the-f-measure/</guid><pubDate>Thu, 10 Sep 2020 21:08:25 GMT</pubDate></item><item><title>Text Data Management and Analysis</title><link>https://necromuralist.github.io/posts/bibliography/text-data-management-and-analysis/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orge6a4998" class="outline-2"&gt;
&lt;h2 id="orge6a4998"&gt;Bibliography&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge6a4998"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Zhai C, Massung S. Text data management and analysis: a practical introduction to information retrieval and text mining. First edition. New York: Association for Computing Machinery; 2016. 510 p. (ACM books).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>bibliography</category><category>information retrieval</category><category>nlp</category><category>text mining</category><guid>https://necromuralist.github.io/posts/bibliography/text-data-management-and-analysis/</guid><pubDate>Thu, 30 Jul 2020 20:23:23 GMT</pubDate></item><item><title>Opinion Mining and Sentiment Analysis</title><link>https://necromuralist.github.io/posts/opinion-mining-and-sentiment-analysis/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org0f2a0fe" class="outline-2"&gt;
&lt;h2 id="org0f2a0fe"&gt;Note&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0f2a0fe"&gt;
&lt;p&gt;
An &lt;i&gt;opinion&lt;/i&gt; is a subjective statement about what a &lt;b&gt;&lt;b&gt;person&lt;/b&gt;&lt;/b&gt; thinks or believes about &lt;b&gt;&lt;b&gt;something&lt;/b&gt;&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
There are three basic parts needed to understand an opinion:
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;The opinion holder (&lt;b&gt;&lt;b&gt;person&lt;/b&gt;&lt;/b&gt;)&lt;/li&gt;
&lt;li&gt;The opinion target (&lt;b&gt;&lt;b&gt;something&lt;/b&gt;&lt;/b&gt;)&lt;/li&gt;
&lt;li&gt;The opinion content&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
In addition, to make it meaningful, you can add:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;The Context of the opinion&lt;/li&gt;
&lt;li&gt;The Sentiment of the opinion&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;a href="https://necromuralist.github.io/posts/bibliography/text-data-management-and-analysis/"&gt;(Zhai &amp;amp; Massung, 2016)&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
Sentiment Analysis is a part of Natural Language Processing that looks to transform freeform text into structured data. Opinion is an expression of belief, while sentiment is an expression of feeling. What is sometimes called Sentiment Analysis - the measurement of positive or negative sentiment of a text is really Polarity Classification, a subset of Sentiment Analysis. &lt;a href="https://necromuralist.github.io/posts/bibliography/sentiment-analysis-in-social-networks/"&gt;(Pozzi, Fersini, et al., 2017)&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>sentiment analysis</category><category>slipnote</category><guid>https://necromuralist.github.io/posts/opinion-mining-and-sentiment-analysis/</guid><pubDate>Thu, 30 Jul 2020 20:12:55 GMT</pubDate></item><item><title>Sentiment Analysis In Social Networks</title><link>https://necromuralist.github.io/posts/bibliography/sentiment-analysis-in-social-networks/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org8d987ca" class="outline-2"&gt;
&lt;h2 id="org8d987ca"&gt;Bibliography&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8d987ca"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Pozzi FA, Fersini E, Messina E, Liu B, editors. Sentiment analysis in social networks. Elsevier Inc.: 2017. 263 p.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>bibliography</category><category>nlp</category><category>sentiment analysis</category><category>text mining</category><guid>https://necromuralist.github.io/posts/bibliography/sentiment-analysis-in-social-networks/</guid><pubDate>Thu, 30 Jul 2020 20:05:58 GMT</pubDate></item><item><title>Speech and Language Processing</title><link>https://necromuralist.github.io/posts/bib-speech-and-language-processing-jurafsky-martin/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org021d1bb" class="outline-2"&gt;
&lt;h2 id="org021d1bb"&gt;Citation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org021d1bb"&gt;
&lt;p&gt;
Jurafsky, D. &amp;amp; Martin, J. (2020). Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition. 3rd Edition draft. &lt;a href="https://web.stanford.edu/~jurafsky/slp3/"&gt;(URL)&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc0e62e4" class="outline-2"&gt;
&lt;h2 id="orgc0e62e4"&gt;Notes&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc0e62e4"&gt;
&lt;p&gt;
Online and PDF version of a (work in progress) revision to this text about text processing.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>bibliography</category><category>nlp</category><guid>https://necromuralist.github.io/posts/bib-speech-and-language-processing-jurafsky-martin/</guid><pubDate>Tue, 28 Jul 2020 03:53:07 GMT</pubDate></item></channel></rss>