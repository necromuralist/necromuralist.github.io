<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Cloistered Monkey (Posts about networking)</title><link>https://necromuralist.github.io/</link><description></description><atom:link href="https://necromuralist.github.io/categories/networking.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Tue, 02 Apr 2019 05:13:37 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Tuning a TCP Connection</title><link>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org2751073"&gt;TCP Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org666156a"&gt;The TCP Congestion Window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org7e7b86d"&gt;Optimal Buffer Size&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orga408832"&gt;Changing the Buffer Size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org942ab22"&gt;Picking a Buffer Size&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgfa991bf"&gt;The Maximum Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgdc7b4f1"&gt;Bandwidth Delay Product&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgc973b3c"&gt;Iperf Tuning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org29785d2"&gt;Iperf Warnings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org34e13cf"&gt;Parallel Streams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org679da2e"&gt;Maximum Transmission Unit (MTU)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
I wrote this a long time ago so I don't remember the source or really the intent. I think it was about trying to maximize the throughput when running iperf over TCP.
&lt;/p&gt;

&lt;div id="outline-container-org2751073" class="outline-2"&gt;
&lt;h2 id="org2751073"&gt;TCP Background&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2751073"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org666156a" class="outline-3"&gt;
&lt;h3 id="org666156a"&gt;The TCP Congestion Window&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org666156a"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Larger window implies higher throughput&lt;/li&gt;
&lt;li&gt;The max allowed is based on the buffer space the kernel allocates for
each socket&lt;/li&gt;
&lt;li&gt;Each socket has a default buffer size&lt;/li&gt;
&lt;li&gt;Sender and receiver can both adjust the size&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7e7b86d" class="outline-3"&gt;
&lt;h3 id="org7e7b86d"&gt;Optimal Buffer Size&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7e7b86d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Too small - Sender will be throttled
&lt;ul class="org-ul"&gt;
&lt;li&gt;More likely if the Sender is faster than the receiver&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Too big - Receiver might be overloaded and packets will be dropped
&lt;ul class="org-ul"&gt;
&lt;li&gt;If the receiver has lots of memory, less likely to happen&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga408832" class="outline-2"&gt;
&lt;h2 id="orga408832"&gt;Changing the Buffer Size&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga408832"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;TCP chooses the smaller of both sides of the transaction.&lt;/li&gt;
&lt;li&gt;Set the receiver high and let the sender negotiate&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org942ab22" class="outline-2"&gt;
&lt;h2 id="org942ab22"&gt;Picking a Buffer Size&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org942ab22"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfa991bf" class="outline-3"&gt;
&lt;h3 id="orgfa991bf"&gt;The Maximum Throughput&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfa991bf"&gt;
&lt;p&gt;
If the network isn't congested the throughput is characterized by TCP buffer size and network latency.
&lt;/p&gt;

\begin{equation*} max \; throughput \gets \frac{buffer \; size}{latency}
\end{equation*}
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdc7b4f1" class="outline-3"&gt;
&lt;h3 id="orgdc7b4f1"&gt;Bandwidth Delay Product&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdc7b4f1"&gt;
&lt;p&gt;
This is a rule of thumb to calculate the optimal buffer size.
&lt;/p&gt;

\begin{equation*} bdp \gets bottleneck \; bandwidth \times round \; trip
\; time \end{equation*}

&lt;ul class="org-ul"&gt;
&lt;li&gt;Get the &lt;b&gt;rtt&lt;/b&gt; from &lt;b&gt;ping&lt;/b&gt;.&lt;/li&gt;
&lt;li&gt;Get &lt;b&gt;bottleneck bandwidth&lt;/b&gt; from the theoretical maximum rate of the slowest link.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc973b3c" class="outline-2"&gt;
&lt;h2 id="orgc973b3c"&gt;Iperf Tuning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc973b3c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org29785d2" class="outline-3"&gt;
&lt;h3 id="org29785d2"&gt;Iperf Warnings&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org29785d2"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Iperf can set the buffer size up to a point – but the OS sets the
upper limit on window sizes.&lt;/li&gt;
&lt;li&gt;If the request is too high, iperf will use the maximum allowed and gives a warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org34e13cf" class="outline-3"&gt;
&lt;h3 id="org34e13cf"&gt;Parallel Streams&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org34e13cf"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Iperf lets you run multiple parallel sessions using the -P flag.&lt;/li&gt;

&lt;li&gt;If the aggregate (SUM) is greater than a single stream, this is an indication that something is wrong – most likely one of:
&lt;ul class="org-ul"&gt;
&lt;li&gt;The TCP window is too small&lt;/li&gt;
&lt;li&gt;The OS implementation has bugs&lt;/li&gt;
&lt;li&gt;The network has problems&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org679da2e" class="outline-3"&gt;
&lt;h3 id="org679da2e"&gt;Maximum Transmission Unit (MTU)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org679da2e"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;the most effective way to set it is if both hosts support Path MTU Discovery and set it themselves.&lt;/li&gt;
&lt;li&gt;iperf's -m flag displays what mss is being used&lt;/li&gt;
&lt;li&gt;&lt;b&gt;mss&lt;/b&gt;: &lt;i&gt;Maximum Segment Size&lt;/i&gt; – the maximum TCP segment size&lt;/li&gt;
&lt;/ul&gt;

\begin{equation*} mss \gets MTU - protocol \; headers \end{equation*}

&lt;ul class="org-ul"&gt;
&lt;li&gt;Using -m is mainly to watch for the warning that the node lacks path mtu discovery&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>networking</category><category>optimization</category><guid>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</guid><pubDate>Mon, 10 Sep 2012 06:51:00 GMT</pubDate></item></channel></rss>