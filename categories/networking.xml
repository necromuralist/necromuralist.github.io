<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Cloistered Monkey (Posts about networking)</title><link>https://necromuralist.github.io/</link><description></description><atom:link href="https://necromuralist.github.io/categories/networking.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Sat, 27 Jul 2019 17:00:38 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Tuning a TCP Connection</title><link>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org8f4a563"&gt;TCP Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgdac7ed1"&gt;The TCP Congestion Window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org07c8dd9"&gt;Optimal Buffer Size&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org1bc640b"&gt;Changing the Buffer Size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orga5f083d"&gt;Picking a Buffer Size&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org814bf75"&gt;The Maximum Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org0a0438c"&gt;Bandwidth Delay Product&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org0462aea"&gt;Iperf Tuning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgd4cab62"&gt;Iperf Warnings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#org9157e22"&gt;Parallel Streams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/201209tuning-tcp-connection/#orgbda730b"&gt;Maximum Transmission Unit (MTU)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
I wrote this a long time ago so I don't remember the source or really the intent. I think it was about trying to maximize the throughput when running iperf over TCP.
&lt;/p&gt;

&lt;div id="outline-container-org8f4a563" class="outline-2"&gt;
&lt;h2 id="org8f4a563"&gt;TCP Background&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8f4a563"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdac7ed1" class="outline-3"&gt;
&lt;h3 id="orgdac7ed1"&gt;The TCP Congestion Window&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdac7ed1"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Larger window implies higher throughput&lt;/li&gt;
&lt;li&gt;The max allowed is based on the buffer space the kernel allocates for
each socket&lt;/li&gt;
&lt;li&gt;Each socket has a default buffer size&lt;/li&gt;
&lt;li&gt;Sender and receiver can both adjust the size&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org07c8dd9" class="outline-3"&gt;
&lt;h3 id="org07c8dd9"&gt;Optimal Buffer Size&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org07c8dd9"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Too small - Sender will be throttled
&lt;ul class="org-ul"&gt;
&lt;li&gt;More likely if the Sender is faster than the receiver&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Too big - Receiver might be overloaded and packets will be dropped
&lt;ul class="org-ul"&gt;
&lt;li&gt;If the receiver has lots of memory, less likely to happen&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1bc640b" class="outline-2"&gt;
&lt;h2 id="org1bc640b"&gt;Changing the Buffer Size&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1bc640b"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;TCP chooses the smaller of both sides of the transaction.&lt;/li&gt;
&lt;li&gt;Set the receiver high and let the sender negotiate&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga5f083d" class="outline-2"&gt;
&lt;h2 id="orga5f083d"&gt;Picking a Buffer Size&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga5f083d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org814bf75" class="outline-3"&gt;
&lt;h3 id="org814bf75"&gt;The Maximum Throughput&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org814bf75"&gt;
&lt;p&gt;
If the network isn't congested the throughput is characterized by TCP buffer size and network latency.
&lt;/p&gt;

\begin{equation*} max \; throughput \gets \frac{buffer \; size}{latency}
\end{equation*}
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0a0438c" class="outline-3"&gt;
&lt;h3 id="org0a0438c"&gt;Bandwidth Delay Product&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0a0438c"&gt;
&lt;p&gt;
This is a rule of thumb to calculate the optimal buffer size.
&lt;/p&gt;

\begin{equation*} bdp \gets bottleneck \; bandwidth \times round \; trip
\; time \end{equation*}

&lt;ul class="org-ul"&gt;
&lt;li&gt;Get the &lt;b&gt;rtt&lt;/b&gt; from &lt;b&gt;ping&lt;/b&gt;.&lt;/li&gt;
&lt;li&gt;Get &lt;b&gt;bottleneck bandwidth&lt;/b&gt; from the theoretical maximum rate of the slowest link.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0462aea" class="outline-2"&gt;
&lt;h2 id="org0462aea"&gt;Iperf Tuning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0462aea"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd4cab62" class="outline-3"&gt;
&lt;h3 id="orgd4cab62"&gt;Iperf Warnings&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd4cab62"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Iperf can set the buffer size up to a point – but the OS sets the
upper limit on window sizes.&lt;/li&gt;
&lt;li&gt;If the request is too high, iperf will use the maximum allowed and gives a warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9157e22" class="outline-3"&gt;
&lt;h3 id="org9157e22"&gt;Parallel Streams&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9157e22"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Iperf lets you run multiple parallel sessions using the -P flag.&lt;/li&gt;

&lt;li&gt;If the aggregate (SUM) is greater than a single stream, this is an indication that something is wrong – most likely one of:
&lt;ul class="org-ul"&gt;
&lt;li&gt;The TCP window is too small&lt;/li&gt;
&lt;li&gt;The OS implementation has bugs&lt;/li&gt;
&lt;li&gt;The network has problems&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbda730b" class="outline-3"&gt;
&lt;h3 id="orgbda730b"&gt;Maximum Transmission Unit (MTU)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbda730b"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;the most effective way to set it is if both hosts support Path MTU Discovery and set it themselves.&lt;/li&gt;
&lt;li&gt;iperf's -m flag displays what mss is being used&lt;/li&gt;
&lt;li&gt;&lt;b&gt;mss&lt;/b&gt;: &lt;i&gt;Maximum Segment Size&lt;/i&gt; – the maximum TCP segment size&lt;/li&gt;
&lt;/ul&gt;

\begin{equation*} mss \gets MTU - protocol \; headers \end{equation*}

&lt;ul class="org-ul"&gt;
&lt;li&gt;Using -m is mainly to watch for the warning that the node lacks path mtu discovery&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>networking</category><category>optimization</category><guid>https://necromuralist.github.io/posts/201209tuning-tcp-connection/</guid><pubDate>Mon, 10 Sep 2012 06:51:00 GMT</pubDate></item></channel></rss>