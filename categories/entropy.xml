<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Cloistered Monkey (Posts about entropy)</title><link>https://necromuralist.github.io/</link><description></description><atom:link href="https://necromuralist.github.io/categories/entropy.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2024 &lt;a href="mailto:cloisteredmonkey.jmark@slmail.me"&gt;Cloistered Monkey&lt;/a&gt; 
&lt;div id="license"xmlns:cc="http://creativecommons.org/ns#" &gt;This work is licensed under
&lt;a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"&gt;CC BY 4.0
&lt;img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"&gt;
&lt;img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"&gt;&lt;/a&gt;
&lt;/div&gt;
</copyright><lastBuildDate>Wed, 01 May 2024 04:52:55 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Decision Tree Entropy</title><link>https://necromuralist.github.io/posts/decision-tree-entropy/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/decision-tree-entropy/#orgfab79c6"&gt;Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/posts/decision-tree-entropy/#org872a3e2"&gt;Dying Young&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
These are some basic notes on how entropy is used when making decision trees. The examples are taken from &lt;i&gt;Process Mining: Data Science In Action&lt;/i&gt;.
&lt;/p&gt;

&lt;div id="outline-container-orgfab79c6" class="outline-2"&gt;
&lt;h2 id="orgfab79c6"&gt;Entropy&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfab79c6"&gt;
&lt;p&gt;
The equation for entropy is \(E = - \sum\limits_{i=1}^n p_i \log_2 p_i\), where \(p_i\) is the probability of variable \(i\). In other words, for variable \(i\), \(p_i\) is the count of instances for that variable divided by the total number of instances for all variables. Well, I'm probably not saying this clearly enough. A concrete example might be better.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org872a3e2" class="outline-2"&gt;
&lt;h2 id="org872a3e2"&gt;Dying Young&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org872a3e2"&gt;
&lt;p&gt;
This example uses a data set that contains various attributes that might predict if someone died 'young' (less than 70) or 'not young' (70 or older). There are 860 entries with 546 dying young and 314 dying old. We can calculate the entropy for the root node using the proportions of \(young\) (died young) and \(\lnot young\) (didn't die young).
&lt;/p&gt;

\begin{align}
  E &amp;amp;= -(E_{young} + E_{\lnot young})\\
  &amp;amp;= -\left(\frac{546}{860} \log_2 \frac{546}{860} + \frac{314}{860} \log_2 \frac{314}{860}\right)\\
  &amp;amp;\approx 0.9468
\end{align}
&lt;/div&gt;
&lt;/div&gt;</description><category>decision trees</category><category>entropy</category><category>machine learning</category><guid>https://necromuralist.github.io/posts/decision-tree-entropy/</guid><pubDate>Mon, 06 Mar 2017 22:14:02 GMT</pubDate></item></channel></rss>