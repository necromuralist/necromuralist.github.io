<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Cloistered Monkey (Posts about learning machinelearning)</title><link>https://necromuralist.github.io/</link><description></description><atom:link href="https://necromuralist.github.io/categories/cat_learning-machinelearning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 13 Feb 2018 20:52:56 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Decision Tree Entropy</title><link>https://necromuralist.github.io/posts/decision-tree-entropy/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;p&gt;
These are some basic notes on how entropy is used when making decision trees. The examples are taken from &lt;i&gt;Process Mining: Data Science In Action&lt;/i&gt;.
&lt;/p&gt;

&lt;div id="outline-container-org9474071" class="outline-2"&gt;
&lt;h2 id="org9474071"&gt;Entropy&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9474071"&gt;
&lt;p&gt;
The equation for entropy is \(E = - \sum\limits_{i=1}^n p_i \log_2 p_i\), where \(p_i\) is the probability of variable \(i\). In other words, for variable \(i\), \(p_i\) is the count of instances for that variable divided by the total number of instances for all variables. Well, I'm probably not saying this clearly enough. A concrete example might be better.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org33be62a" class="outline-2"&gt;
&lt;h2 id="org33be62a"&gt;Dying Young&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org33be62a"&gt;
&lt;p&gt;
This example uses a data set that contains various attributes that might predict if someone died 'young' (less than 70) or 'not young' (70 or older). There are 860 entries with 546 dying young and 314 dying old. We can calculate the entropy for the root node using the proportions of \(young\) (died young) and \(\lnot young\) (didn't die young).
&lt;/p&gt;

&lt;p&gt;
\[
  E = -(E_{young} + E_{\lnot young})\\
  = -(\frac{546}{860} \log_2 \frac{546}{860} + \frac{314}{860} \log_2 \frac{314}{860})\\
  \approx 0.9468
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>machineLearning entropy decisionTrees</category><guid>https://necromuralist.github.io/posts/decision-tree-entropy/</guid><pubDate>Mon, 06 Mar 2017 22:14:02 GMT</pubDate></item></channel></rss>